{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json, pandas as pd, numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.spatial.distance import euclidean\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are our configuration variables\n",
    "file_name = \"training_dataset_two.json\"\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser as dateparser\n",
    "#This is used to parse the date to a datetime object.\n",
    "def parse_effective_date(date_str):\n",
    "    try:\n",
    "        return dateparser.parse(date_str)\n",
    "    except (ValueError, TypeError):\n",
    "        return datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to parse the GLA (Gross Living Area) value.\n",
    "import re\n",
    "\n",
    "def safe_parse_gla(gla_value):\n",
    "    if not gla_value:\n",
    "        return 0.0\n",
    "    try:\n",
    "        # Extract only the number\n",
    "        cleaned = re.sub(r\"[^\\d.]\", \"\", str(gla_value))\n",
    "        return float(cleaned)\n",
    "    except (ValueError, TypeError):\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are defining a function to calculate the age of the property.\n",
    "def get_age(year_built_or_age, effective_year):\n",
    "    try:\n",
    "        val = int(float(str(year_built_or_age).strip()))\n",
    "        if 0 < val < 150:\n",
    "            return val  # use directly if it's an age\n",
    "        elif val >= 1800:\n",
    "            return max(effective_year - val, 0)\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_property_data(subject, properties, effective_date_str):\n",
    "    effective_date = parse_effective_date(effective_date_str)\n",
    "    effective_year = effective_date.year if isinstance(effective_date, datetime) else datetime.now().year\n",
    "\n",
    "    procecessed_props = []\n",
    "\n",
    "    # Here we will process the subject's key property characterisitics.\n",
    "    subject_features = {\n",
    "        'id':'subject',\n",
    "        'gla': safe_parse_gla(subject.get(\"gla\")),\n",
    "        \"rooms\": subject.get(\"room_total\", 0),\n",
    "        \"age\": get_age(subject.get(\"year_built\", \"N/A\"), effective_year),\n",
    "        \"structure_type\": str(subject.get(\"structure_type\"))\n",
    "    }\n",
    "    procecessed_props.append(subject_features)\n",
    "\n",
    "    # Here we will process the key property characterisitics of the properties.\n",
    "    for i, property in enumerate(properties):\n",
    "        prop_features = {\n",
    "            'id': property.get(\"id\", f\"property_{i}\"),\n",
    "            'gla': safe_parse_gla(property.get(\"gla\")),\n",
    "            \"rooms\": property.get(\"room_total\", 0),\n",
    "            #\"age\": get_age(property.get(\"year_built\", \"N/A\"), effective_year),\n",
    "            \"structure_type\": str(property.get(\"structure_type\"))\n",
    "        }\n",
    "        procecessed_props.append(prop_features)\n",
    "    \n",
    "    df = pd.DataFrame(procecessed_props)\n",
    "\n",
    "    # Here we will process the key property characterisitics of the comps.\n",
    "    df = pd.get_dummies(df, columns=[\"structure_type\"], drop_first=True)\n",
    "\n",
    "    # Here we're splitting the dataset into two parts: the subject and the properties.\n",
    "    subject_df = df[df['id'] == 'subject'].copy()\n",
    "    properties_df = df[df['id'] != 'subject'].copy()\n",
    "\n",
    "    # Here we are splitting the columns into two parts: the numerical features and the one-hot encoded features.\n",
    "    numerical_features = ['gla', 'rooms', 'age']\n",
    "    one_hot_columns = [col for col in df.columns if col.startswith('structure_type_')]\n",
    "    # Here we're combining the numerical features and the one-hot encoded features into a single list.\n",
    "    final_feature_columns = numerical_features + one_hot_columns\n",
    "\n",
    "    # Here we're ensuring that all columns are present in both the subject and properties dataframes.\n",
    "    all_structure_cols = set(df.columns[df.columns.str.startswith(\"structure_type_\")])\n",
    "    for col in all_structure_cols:\n",
    "        if col not in subject_df.columns:\n",
    "            subject_df[col] = 0\n",
    "        if col not in properties_df.columns:\n",
    "            properties_df[col] = 0\n",
    "    \n",
    "    # Here we're reordering the columns to match the final feature columns.\n",
    "    subject_df = subject_df[final_feature_columns]\n",
    "    candidate_df = properties_df[final_feature_columns]\n",
    "\n",
    "    # Here we're using the scaler to scale the data. It will be between 0 and 1 and could change based on the minimum and maximum values of the dataset.\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(pd.concat([subject_df, candidate_df], ignore_index=True))\n",
    "\n",
    "    # Here we're applying the scaler to the subject and properties dataframes.\n",
    "    # This will scale the data to be between 0 and 1 and to ensure that features like GLA and room count are equally important.\n",
    "    subject_scaled_df = pd.DataFrame(scaler.transform(subject_df), columns=final_feature_columns, index = subject_df.index)\n",
    "    candidates_scaled_df = pd.DataFrame(scaler.transform(candidate_df), columns=final_feature_columns, index = candidate_df.index)\n",
    "\n",
    "    # Here we're adding the original id to the scaled dataframe for the properties.\n",
    "    # This would be used to identify the properties in the original dataset.\n",
    "    candidates_scaled_df['original_id'] = properties_df['id'].values\n",
    "    candidates_scaled_df['address'] = properties_df.get('address', pd.Series([\"\"] * len(properties_df)))\n",
    "\n",
    "    # Here we're returning the row representing the subject property, the scaled properties dataframe, and the list of features used for distance calculation (euclidean distance calculations).\n",
    "    return subject_scaled_df.iloc[0], candidates_scaled_df, final_feature_columns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_properties(file_name, k):\n",
    "    with open(file_name) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Here we're extracting the subject property and the properties from the JSON data.\n",
    "    subject = data.get(\"subject\", [])\n",
    "    properties = data.get(\"properties\", [])\n",
    "    comps = data.get(\"comps\", [])\n",
    "    all_candidates = properties + comps\n",
    "\n",
    "    # Here we're extracting the effective date from the subject property.\n",
    "    effective_date_str = subject.get(\"effective_date\", \"\")\n",
    "\n",
    "    #Preprocess the data\n",
    "    subject_scaled, candidates_scaled, final_feature_columns = preprocess_property_data(subject, all_candidates, effective_date_str)\n",
    "\n",
    "    # Here we're defining the weights for the features used in the distance calculation.\n",
    "    weights = {\n",
    "        'gla': 1.5,\n",
    "        'rooms': 1.5,\n",
    "        'age': 1.5\n",
    "    }\n",
    "    # Here we're adding the weights for the one-hot encoded columns.\n",
    "    for col in final_feature_columns:\n",
    "        if col not in weights:\n",
    "            weights[col] = 0.2  # default weight for one-hot columns\n",
    "\n",
    "    #Calculating the distances\n",
    "    distances = []\n",
    "    subject_vector = subject_scaled[final_feature_columns].values.astype(float) \n",
    "    subject_vector *= np.array([weights[col] for col in final_feature_columns]) # Apply weights to the subject vector\n",
    "\n",
    "    # Here we're iterating over the candidates and calculating the euclidean distance between the subject and each candidate.\n",
    "    for index , row in candidates_scaled.iterrows():\n",
    "        candidate_vector = row[final_feature_columns].values.astype(float)\n",
    "        # Here we're applying the weights to the candidate vector.\n",
    "        dist = euclidean(subject_vector, candidate_vector)\n",
    "        distances.append({\n",
    "            'id': row['original_id'],\n",
    "            'distance': dist,\n",
    "            'address': row['address']\n",
    "        })\n",
    "    \n",
    "    #Putting the distances in a dataframe\n",
    "    distances_df = pd.DataFrame(distances)\n",
    "    # Here we're sorting the distances in ascending order.\n",
    "    sorted_distances_df = distances_df.sort_values(by='distance', ascending=True)\n",
    "    # Here we're resetting the index of the sorted distances dataframe.\n",
    "    print (f\"\\nTop {k} most similar properties\")\n",
    "    top_k_properties = []\n",
    "    output_rows = []\n",
    "    for i in range(min(k, len(sorted_distances_df))):\n",
    "       # Here we're getting the top 3 properties based on the sorted distances.\n",
    "       neighbour = sorted_distances_df.iloc[i]\n",
    "       # Here we're getting the original property details from the properties list.\n",
    "       original_property_detail = next((prop for prop in properties if prop.get(\"id\") == neighbour['id']), None)\n",
    "       is_comp = any(str(comp.get(\"id\")) == str(neighbour['id']) for comp in comps)\n",
    "\n",
    "        # Here we're printing the details of the top 3 properties.\n",
    "       print (f\"\\nRank {i+1}:\")\n",
    "       print (f\"Property ID: {neighbour['id']}\")\n",
    "       print (f\"Distance: {neighbour['distance']}\")\n",
    "       print (f\"Address: {neighbour['address']}\")\n",
    "       print (f\"Is Comp: {'yes' if is_comp else 'no'}\")\n",
    "       if original_property_detail:\n",
    "           print (f\"Property Details: {json.dumps(original_property_detail, indent=2)}\")\n",
    "           # Here we're creating a row for the output dataframe.\n",
    "           row = {\n",
    "                'rank': i + 1,\n",
    "                'id': neighbour['id'],\n",
    "                'distance': neighbour['distance'],\n",
    "                'address': neighbour['address'],\n",
    "                'is_comp': is_comp,\n",
    "                'property_details': original_property_detail,\n",
    "                'gla': original_property_detail.get(\"gla\"), # type: ignore\n",
    "                'Bedrooms': original_property_detail.get(\"room_total\", 0), # type: ignore\n",
    "           }\n",
    "           # Here we're appending the row to the output rows.\n",
    "           output_rows.append(row)\n",
    "           top_k_properties.append(original_property_detail)\n",
    "\n",
    "    # Here we're saving the output to a CSV file.\n",
    "    output_df = pd.DataFrame(output_rows)\n",
    "    output_df.to_csv('output.csv', index=False)\n",
    "    print (f\"\\nOutput saved to output.csv\")\n",
    "\n",
    "    # Here we're returning the top 3 properties.\n",
    "    return top_k_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning the software recommendation process...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Call the function to find similar properties\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBeginning the software recommendation process...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     similar_props \u001b[38;5;241m=\u001b[39m find_similar_properties(file_name, k)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m similar_props:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(similar_props)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m similar properties.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[58], line 37\u001b[0m, in \u001b[0;36mfind_similar_properties\u001b[1;34m(file_name, k)\u001b[0m\n\u001b[0;32m     35\u001b[0m     candidate_vector \u001b[38;5;241m=\u001b[39m row[final_feature_columns]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# Here we're applying the weights to the candidate vector.\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     dist \u001b[38;5;241m=\u001b[39m euclidean(subject_vector, candidate_vector)\n\u001b[0;32m     38\u001b[0m     distances\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_id\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m'\u001b[39m: dist,\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddress\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddress\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     42\u001b[0m     })\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m#Putting the distances in a dataframe\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jai ma\\anaconda3\\Lib\\site-packages\\scipy\\spatial\\distance.py:520\u001b[0m, in \u001b[0;36meuclidean\u001b[1;34m(u, v, w)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meuclidean\u001b[39m(u, v, w\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    485\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03m    Computes the Euclidean distance between two 1-D arrays.\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    518\u001b[0m \n\u001b[0;32m    519\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m minkowski(u, v, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, w\u001b[38;5;241m=\u001b[39mw)\n",
      "File \u001b[1;32mc:\\Users\\jai ma\\anaconda3\\Lib\\site-packages\\scipy\\spatial\\distance.py:480\u001b[0m, in \u001b[0;36mminkowski\u001b[1;34m(u, v, p, w)\u001b[0m\n\u001b[0;32m    478\u001b[0m         root_w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpower(w, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mp)\n\u001b[0;32m    479\u001b[0m     u_v \u001b[38;5;241m=\u001b[39m root_w \u001b[38;5;241m*\u001b[39m u_v\n\u001b[1;32m--> 480\u001b[0m dist \u001b[38;5;241m=\u001b[39m norm(u_v, \u001b[38;5;28mord\u001b[39m\u001b[38;5;241m=\u001b[39mp)\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dist\n",
      "File \u001b[1;32mc:\\Users\\jai ma\\anaconda3\\Lib\\site-packages\\scipy\\linalg\\_misc.py:146\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(a, ord, axis, keepdims, check_finite)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# Differs from numpy only in non-finite handling and the use of blas.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_finite:\n\u001b[1;32m--> 146\u001b[0m     a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray_chkfinite(a)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(a)\n",
      "File \u001b[1;32mc:\\Users\\jai ma\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:630\u001b[0m, in \u001b[0;36masarray_chkfinite\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    628\u001b[0m a \u001b[38;5;241m=\u001b[39m asarray(a, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mchar \u001b[38;5;129;01min\u001b[39;00m typecodes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllFloat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(a)\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m--> 630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray must not contain infs or NaNs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[1;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Call the function to find similar properties\n",
    "    print (\"Beginning the software recommendation process...\")\n",
    "    similar_props = find_similar_properties(file_name, k)\n",
    "    if similar_props:\n",
    "        print (f\"\\nFound {len(similar_props)} similar properties.\")\n",
    "    else:\n",
    "        print (\"No similar properties found.\")\n",
    "\n",
    "# This is the main function that will be called when the script is run.\n",
    "find_similar_properties(file_name, k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
